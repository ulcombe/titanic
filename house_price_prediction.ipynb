{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "e81ee64d-e474-4662-9036-ce23df615199",
        "_uuid": "b6269c0e8f417f82daf093dda8fa0da6d2c57d86"
      },
      "cell_type": "markdown",
      "source": "# Machine Learning Part 1 tutorial\n"
    },
    {
      "metadata": {
        "_cell_guid": "86b26423-563a-4fa1-a595-89e25ff93089",
        "_uuid": "1c728098629e1301643443b1341556a15c089b2b",
        "trusted": true,
        "_kg_hide-input": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\nmain_file_path = '../input/train.csv' # this is the path to the Iowa data that you will use\ndata = pd.read_csv(main_file_path)\n\nprint(data.mean()['LotArea'])\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3e26971e1ec8b0362b1194b83b1b064dbc44aaea"
      },
      "cell_type": "markdown",
      "source": "# Select predictors for model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db7e5b2104700a40d2e9e0f510d237e68e65bcf7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# melbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n#                        'YearBuilt', 'Lattitude', 'Longtitude']\ncolumns_of_interest = ['LotArea',\n'LotFrontage',\n'YearBuilt',\n'1stFlrSF',\n'2ndFlrSF',\n'FullBath',\n'BedroomAbvGr',\n'TotRmsAbvGrd',\n'MSSubClass',\n'YrSold']\n#columns_of_interest = ['MSSubClass','LotFrontage','LotArea','YearBuilt',\n#                      'OverallQual','TotRmsAbvGrd']\n# columns_of_interest = ['LotArea', 'LotShape']\nX = data[columns_of_interest]\ny = data.SalePrice\n#print(data.iloc[1:3,1:7])\n#print(data.iloc[1:3,8:14])\n#print(data.iloc[1:3,15:20])\n#print(data.iloc[1:3,21:25])\nprint(data.columns)\n#X.fillna(value=0, inplace=True)\n#X.isna()\nX.describe()\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51b73aff80d540e17c7af512f3291e690de43b0d"
      },
      "cell_type": "markdown",
      "source": "# Impute missing values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f67157908bbd95a8c49acc10ac518f86a9a68265",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.impute import SimpleImputer\n\nX_original = X.copy()\nmy_imputer = SimpleImputer(missing_values='NaN', strategy='mean')\n#print(X.columns)\nX = pd.DataFrame(my_imputer.fit_transform(X),columns=X.columns)  \n\n# make new columns indicating what will be imputed\n# cols_with_missing = (col for col in X.columns() \n#                                 if X[c].isnull().any())\n# for col in cols_with_missing:\n#     X[col + '_was_missing'] = X[col].isnull()\n\n# Imputation\n#my_imputer = Imputer()\n#X = my_imputer.fit_transform(X)\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "70df621d19458cedc1a5bd836ee653fd4a812d7c"
      },
      "cell_type": "markdown",
      "source": "# Create DecisionTree"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3fde9f485831bd75983ef4288a018ef2419fb04",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeRegressor\n\n# Define model\nmodel = DecisionTreeRegressor()\n\n# Fit model\nmodel.fit(X, y)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "881b0459954822654f460190bf5a8d646d569fbe",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(model.predict(X.head()))",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b0cbe10545c6c0782218be3943112fd8ccdfe786"
      },
      "cell_type": "markdown",
      "source": "# Test model accuracy"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "859caf26e1b62cdebee7d13812caa254fe82aac7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = model.predict(X)\nmean_absolute_error(y, predicted_home_prices)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "966d7fad7ec1f6eecd1fcd2e3229ff4143d963d4"
      },
      "cell_type": "markdown",
      "source": "# Repeat Model splitting into test dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4024aedeb168eade6ceff288d4cda4eccace87b0",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2996321552123b1791015fe2d563879b9bf32809"
      },
      "cell_type": "markdown",
      "source": "## Determine best MAE based on max leaf nodes"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d245e3015ed14541d4a4dde90246ca23b04b1128"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa1ca1461917b2e06ba8361e0e98dd8ca6d2c038",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f0942c44e05f8c6d5869b1ada72aeb2cdb8371eb"
      },
      "cell_type": "markdown",
      "source": "# Create RandomForest model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35211112e2ecc27c95089117640f09abc63db948",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bab740e88887537705c7823c53557e845a88439"
      },
      "cell_type": "markdown",
      "source": "# Test model for Kaggle submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29c6234617560f8416d6bfa000fe734396247b52",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Read the test data\ntest = pd.read_csv('../input/test.csv')\n\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntest_X = test[columns_of_interest]\n\n#print(X.columns)\ntrain_X = pd.DataFrame(my_imputer.fit_transform(train_X),columns=train_X.columns)  \ntest_X = pd.DataFrame(my_imputer.fit_transform(test_X),columns=test_X.columns)  \n\n# Use the model to make predictions\npredicted_prices = forest_model.predict(test_X)\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_prices)",
      "execution_count": 116,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "355636a8a5758321b651eb4a32dc9ec58434c548"
      },
      "cell_type": "markdown",
      "source": "# XG Boost"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92f806cc2aff156dbe3aef1d96fd73ce3ae0bf5d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from xgboost import XGBRegressor\n\ndata = pd.read_csv('../input/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25)\n\nxgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n# Add silent=True to avoid printing out updates with each cycle\nxgb_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)],verbose=False)\n\n#my_imputer = SimpleImputer(missing_values='NaN', strategy='mean')\n#train_X = pd.DataFrame(my_imputer.fit_transform(train_X),columns=train_X.columns)  \n\n# make predictions\n#xgb_predictions = xgb_model.predict(test_X)\n\n#from sklearn.metrics import mean_absolute_error\n#print(\"Mean Absolute Error : \" + str(mean_absolute_error(xgb_predictions, val_y)))\n",
      "execution_count": 131,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39bcd466f6c25516d0339eb51520b3d50ccc6780",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test = pd.read_csv('../input/test.csv')\nprint(test.shape)\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntest_X = test #[columns_of_interest]\ntest_X = test.select_dtypes(exclude=['object'])\n\n#print(test_X.columns)\nprint(test_X.shape)\n#test_X = pd.DataFrame(my_imputer.fit_transform(test_X),columns=test_X.columns)  \nprint(test_X.shape)\n\n# Use the model to make predictions\nxgb_predicted_prices = xgb_model.predict(test_X)\nprint(xgb_predicted_prices)\n",
      "execution_count": 132,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28304fe9f12b4900970e7f7b8cfbde104fe0c1cd"
      },
      "cell_type": "markdown",
      "source": "# One hot encoding"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b82d3d912a1c7e7952316bcbde216503c1dc8205",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\n# print(train_X.dtypes.sample(10))\none_hot_encoded_training_predictors = pd.get_dummies(train_X)\npredictors_without_categoricals = train_X.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, train_y)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, train_y)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))\n\none_hot_encoded_test_predictors = pd.get_dummies(test_X)\nprint(test_X.head())\nprint(one_hot_encoded_test_predictors.head())\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\nforest_model = RandomForestRegressor()\nforest_model.fit(one_hot_encoded_training_predictors, train_y)\n\n# Use the model to make predictions\npredicted_prices = forest_model.predict(final_test)\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_prices)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e4885975055bda9fbb3315165915d01183b5153",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': xgb_predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)",
      "execution_count": 133,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b8796366683a1b365decdd70d4b5b3b62ab21e76"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}